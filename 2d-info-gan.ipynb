{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "TRAIN = 0\n",
    "EVAL = 1\n",
    "PRED = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "reshaped_X = tf.reshape(X,[-1,28,28,1])\n",
    "mode = tf.placeholder(tf.int32,shape = ())\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 16])\n",
    "c = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "'''\n",
    "D_W1 = tf.Variable(xavier_init([784, 128]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 16])\n",
    "c = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([26, 256]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[256]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([256, 784]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "\n",
    "Q_W1 = tf.Variable(xavier_init([784, 128]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "Q_W2 = tf.Variable(xavier_init([128, 10]))\n",
    "Q_b2 = tf.Variable(tf.zeros(shape=[10]))\n",
    "\n",
    "theta_Q = [Q_W1, Q_W2, Q_b1, Q_b2]\n",
    "'''\n",
    "xavier_init = tf.contrib.layers.xavier_initializer(uniform=False)\n",
    "\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "\n",
    "def sample_c(m):\n",
    "    return np.random.multinomial(1, 10*[0.1], size=m)\n",
    "\n",
    "def shared_layers(input_layer):\n",
    "    with tf.variable_scope(\"shared_conv\",reuse = tf.AUTO_REUSE) as scope:\n",
    "        conv1 = tf.layers.conv2d(\n",
    "          inputs=input_layer,\n",
    "          filters=32,\n",
    "          kernel_size=[5, 5],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu,\n",
    "          name = \"shared_conv1\",\n",
    "          reuse = tf.AUTO_REUSE,\n",
    "          kernel_initializer=xavier_init\n",
    "        )\n",
    "    \n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "        conv2 = tf.layers.conv2d(\n",
    "          inputs=pool1,\n",
    "          filters=64,\n",
    "          kernel_size=[5, 5],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu,\n",
    "          name = \"shared_conv2\",\n",
    "          reuse = tf.AUTO_REUSE,\n",
    "          kernel_initializer=xavier_init\n",
    "        )\n",
    "    \n",
    "        pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "        pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    return pool2_flat\n",
    "\n",
    "def generator(z, c,mode):\n",
    "    #inputs = tf.concat(axis=1, values=[z, c])\n",
    "    #G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "    #G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    #G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "    with tf.variable_scope(\"generator\") as scope:\n",
    "        input_layer = tf.concat(axis=1,values = [z,c])\n",
    "        dense_layer = tf.layers.dense(\n",
    "            inputs = input_layer, \n",
    "            units = 7 * 7 * 64, \n",
    "            activation = tf.nn.relu,\n",
    "            kernel_initializer=xavier_init\n",
    "        )\n",
    "        deconv1 = tf.reshape(dense_layer,[-1,7,7,64])\n",
    "        print(deconv1.get_shape())\n",
    "        norm_deconv1 = tf.layers.batch_normalization(\n",
    "            deconv1, \n",
    "            training=mode==TRAIN,\n",
    "            momentum=0.8\n",
    "        )\n",
    "        '''\n",
    "        unpooling1 = tf.image.resize_images(\n",
    "            images = norm_deconv1,\n",
    "            size = tf.constant([14,14])\n",
    "        )\n",
    "        '''\n",
    "        deconv2 = tf.layers.conv2d_transpose(\n",
    "            inputs = norm_deconv1,\n",
    "            filters = 32,\n",
    "            kernel_size = [5,5],\n",
    "            padding = \"same\",\n",
    "            kernel_initializer=xavier_init,\n",
    "            activation = tf.nn.relu,\n",
    "            strides = 2\n",
    "        )\n",
    "        print(deconv2.get_shape())\n",
    "        norm_deconv2 = tf.layers.batch_normalization(\n",
    "            deconv2, \n",
    "            training=mode==TRAIN,\n",
    "            momentum=0.8\n",
    "        )\n",
    "        '''\n",
    "        unpooling2 = tf.image.resize_images(\n",
    "            images = norm_deconv2,\n",
    "            size = tf.constant([28,28])\n",
    "        )\n",
    "        '''\n",
    "        deconv3 = tf.layers.conv2d_transpose(\n",
    "            inputs = norm_deconv2,\n",
    "            filters = 1,\n",
    "            kernel_size = [2,2],\n",
    "            padding = \"same\",\n",
    "            kernel_initializer=xavier_init,\n",
    "            activation = tf.nn.relu,\n",
    "            strides = 2\n",
    "        )\n",
    "        print(deconv3.get_shape())\n",
    "        norm_deconv3 = tf.layers.batch_normalization(\n",
    "            deconv3, \n",
    "            training=mode==TRAIN,\n",
    "            momentum=0.8\n",
    "        )\n",
    "    return norm_deconv3\n",
    "\n",
    "\n",
    "def discriminator(x,D_mode):\n",
    "    #D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    #D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    #D_prob = tf.nn.sigmoid(D_logit)\n",
    "    pool2_flat = shared_layers(x)\n",
    "    with tf.variable_scope(\"discriminator\",reuse = tf.AUTO_REUSE) as scope:\n",
    "        dense = tf.layers.dense(\n",
    "            inputs=pool2_flat, \n",
    "            units=1024, \n",
    "            activation=tf.nn.relu,\n",
    "            reuse = tf.AUTO_REUSE,\n",
    "            name = \"discriminator_dense1\",\n",
    "            kernel_initializer=xavier_init\n",
    "        )\n",
    "        dropout = tf.layers.dropout(\n",
    "            inputs=dense, \n",
    "            rate=0.5, \n",
    "            training= D_mode == TRAIN\n",
    "        )\n",
    "        D_logit = tf.layers.dense(\n",
    "            inputs=dropout,\n",
    "            units=1,\n",
    "            reuse = tf.AUTO_REUSE, \n",
    "            name = \"discriminator_dense2\",\n",
    "            kernel_initializer=xavier_init\n",
    "        )\n",
    "        D_prob = tf.nn.sigmoid(D_logit)\n",
    "    return D_prob\n",
    "\n",
    "\n",
    "def Q(x,Q_mode):\n",
    "    #Q_h1 = tf.nn.relu(tf.matmul(x, Q_W1) + Q_b1)\n",
    "    #Q_prob = tf.nn.softmax(tf.matmul(Q_h1, Q_W2) + Q_b2)\n",
    "    pool2_flat = shared_layers(x)\n",
    "    with tf.variable_scope(\"Q\") as scope:\n",
    "        dense = tf.layers.dense(\n",
    "            inputs=pool2_flat, \n",
    "            units=1024, \n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=xavier_init\n",
    "        )\n",
    "        dropout = tf.layers.dropout(\n",
    "            inputs=dense, \n",
    "            rate=0.5, \n",
    "            training= Q_mode == TRAIN\n",
    "        )\n",
    "        Q_logit = tf.layers.dense(inputs=dropout, units=10)\n",
    "        Q_prob = tf.nn.softmax(Q_logit)\n",
    "        \n",
    "    return Q_prob\n",
    "\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7, 7, 64)\n",
      "(?, 14, 14, 32)\n",
      "(?, 28, 28, 1)\n",
      "Extracting ../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter: 0\n",
      "D loss: 1.39\n",
      "G_loss: 0.6555\n",
      "\n",
      "Iter: 1000\n",
      "D loss: 0.2426\n",
      "G_loss: 3.507\n",
      "\n",
      "Iter: 2000\n",
      "D loss: 0.6602\n",
      "G_loss: 2.231\n",
      "\n",
      "Iter: 3000\n",
      "D loss: 0.8277\n",
      "G_loss: 2.091\n",
      "\n",
      "Iter: 4000\n",
      "D loss: 0.9544\n",
      "G_loss: 2.731\n",
      "\n",
      "Iter: 5000\n",
      "D loss: 0.486\n",
      "G_loss: 3.459\n",
      "\n",
      "Iter: 6000\n",
      "D loss: 0.3591\n",
      "G_loss: 2.949\n",
      "\n",
      "Iter: 7000\n",
      "D loss: 0.7784\n",
      "G_loss: 1.059\n",
      "\n",
      "Iter: 8000\n",
      "D loss: 0.5825\n",
      "G_loss: 3.659\n",
      "\n",
      "Iter: 9000\n",
      "D loss: 0.4602\n",
      "G_loss: 3.594\n",
      "\n",
      "Iter: 10000\n",
      "D loss: 0.2393\n",
      "G_loss: 4.82\n",
      "\n",
      "Iter: 11000\n",
      "D loss: 0.1676\n",
      "G_loss: 4.095\n",
      "\n",
      "Iter: 12000\n",
      "D loss: 0.09591\n",
      "G_loss: 18.07\n",
      "\n",
      "Iter: 13000\n",
      "D loss: 0.5005\n",
      "G_loss: 4.35\n",
      "\n",
      "Iter: 14000\n",
      "D loss: 0.00027\n",
      "G_loss: 15.04\n",
      "\n",
      "Iter: 15000\n",
      "D loss: 0.0001604\n",
      "G_loss: 11.75\n",
      "\n",
      "Iter: 16000\n",
      "D loss: 0.003431\n",
      "G_loss: 10.12\n",
      "\n",
      "Iter: 17000\n",
      "D loss: 0.3149\n",
      "G_loss: 4.58\n",
      "\n",
      "Iter: 18000\n",
      "D loss: 0.547\n",
      "G_loss: 4.695\n",
      "\n",
      "Iter: 19000\n",
      "D loss: 0.7193\n",
      "G_loss: 5.083\n",
      "\n",
      "Iter: 20000\n",
      "D loss: 0.1881\n",
      "G_loss: 7.768\n",
      "\n",
      "Iter: 21000\n",
      "D loss: 0.0001893\n",
      "G_loss: 17.74\n",
      "\n",
      "Iter: 22000\n",
      "D loss: 0.1124\n",
      "G_loss: 6.349\n",
      "\n",
      "Iter: 23000\n",
      "D loss: 0.06163\n",
      "G_loss: 5.034\n",
      "\n",
      "Iter: 24000\n",
      "D loss: 0.2661\n",
      "G_loss: 3.52\n",
      "\n",
      "Iter: 25000\n",
      "D loss: 0.1782\n",
      "G_loss: 16.05\n",
      "\n",
      "Iter: 26000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 27000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 28000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 29000\n",
      "D loss: 3.353e-08\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 30000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 31000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 32000\n",
      "D loss: 3.725e-09\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 33000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 34000\n",
      "D loss: 1.773e-05\n",
      "G_loss: 18.34\n",
      "\n",
      "Iter: 35000\n",
      "D loss: 1.971e-06\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 36000\n",
      "D loss: 3.725e-09\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 37000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 38000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 39000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 40000\n",
      "D loss: 5.812e-07\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 41000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 42000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 43000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 44000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 45000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 46000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 47000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 48000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 49000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 50000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 51000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 52000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 53000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 54000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 55000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 56000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 57000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 58000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 59000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 60000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 61000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 62000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 63000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 64000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 65000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 66000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 67000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 68000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 69000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 70000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 71000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 72000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 73000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 74000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 75000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 76000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 77000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 78000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 79000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 80000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 81000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 82000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 83000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 84000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 85000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 86000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 87000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 88000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 89000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 90000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 91000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 92000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 93000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 94000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 95000\n",
      "D loss: 0.5672\n",
      "G_loss: 18.42\n",
      "\n",
      "Iter: 96000\n",
      "D loss: 1.527e-07\n",
      "G_loss: 18.28\n",
      "\n",
      "Iter: 97000\n",
      "D loss: -0.0\n",
      "G_loss: 18.42\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9e655265be3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     _, G_loss_curr = sess.run([G_solver, G_loss],\n\u001b[0;32m---> 59\u001b[0;31m                               feed_dict={Z: Z_noise, c: c_noise})\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mQ_solver\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mZ_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc_noise\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "G_sample = generator(Z, c,mode)\n",
    "D_real = discriminator(reshaped_X,mode)\n",
    "D_fake = discriminator(G_sample,mode)\n",
    "Q_c_given_x = Q(G_sample,mode)\n",
    "\n",
    "D_loss = -tf.reduce_mean(tf.log(D_real + 1e-8) + tf.log(1 - D_fake + 1e-8))\n",
    "G_loss = -tf.reduce_mean(tf.log(D_fake + 1e-8))\n",
    "\n",
    "cross_ent = tf.reduce_mean(-tf.reduce_sum(tf.log(Q_c_given_x + 1e-8) * c, 1))\n",
    "ent = tf.reduce_mean(-tf.reduce_sum(tf.log(c + 1e-8) * c, 1))\n",
    "Q_loss = cross_ent + ent\n",
    "\n",
    "theta_shared = tf.trainable_variables(scope=\"shared_conv\")\n",
    "theta_D  = tf.trainable_variables(scope=\"discriminator\")\n",
    "theta_G  = tf.trainable_variables(scope=\"generator\")\n",
    "theta_Q  = tf.trainable_variables(scope=\"Q\")\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D+theta_shared)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "Q_solver = tf.train.AdamOptimizer().minimize(Q_loss, var_list=theta_G + theta_Q+theta_shared)\n",
    "\n",
    "mb_size = 32\n",
    "Z_dim = 16\n",
    "\n",
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('2d_out/'):\n",
    "    os.makedirs('2d_out/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(1000000):\n",
    "    if it % 1000 == 0:\n",
    "        Z_noise = sample_Z(16, Z_dim)\n",
    "\n",
    "        idx = np.random.randint(0, 10)\n",
    "        c_noise = np.zeros([16, 10])\n",
    "        c_noise[range(16), idx] = 1\n",
    "\n",
    "        samples = sess.run(G_sample,\n",
    "                           feed_dict={Z: Z_noise, c: c_noise,mode:PRED})\n",
    "\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('2d_out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)\n",
    "\n",
    "    X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "    Z_noise = sample_Z(mb_size, Z_dim)\n",
    "    c_noise = sample_c(mb_size)\n",
    "\n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss],\n",
    "                              feed_dict={X: X_mb, Z: Z_noise, c: c_noise,mode:TRAIN})\n",
    "\n",
    "    _, G_loss_curr = sess.run([G_solver, G_loss],\n",
    "                              feed_dict={Z: Z_noise, c: c_noise,mode:TRAIN})\n",
    "\n",
    "    sess.run([Q_solver], feed_dict={Z: Z_noise, c: c_noise})\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('D loss: {:.4}'. format(D_loss_curr))\n",
    "        print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
